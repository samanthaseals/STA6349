---
title: "Approximating the Posterior: Part 1"
subtitle: "STA6990: Applied Bayesian Analysis"
execute:
  echo: true
  warning: false
  message: false
  error: false
format: 
  revealjs:
    theme: uwf2
    embed-resources: true
    slide-number: false
    width: 1600
    height: 900
    df-print: paged
    html-math-method: katex
title-slide-attributes:
    data-background-image: /Users/sseals/Library/CloudStorage/GoogleDrive-sseals@uwf.edu/My Drive/00 - Personal/R/quarto themes/title.png
    data-background-size: contain 
editor: source
pdf-separate-fragments: true
fig-align: center
---

## Introduction {background-image="/Users/sseals/Library/CloudStorage/GoogleDrive-sseals@uwf.edu/My Drive/00 - Personal/R/quarto themes/slide.png" background-size="contain"} 

- We have learned how to think like a Bayesian: 

    - Prior distribution
    - Data distribution
    - Posterior distribution

- We have learned three conjugate families:

    - Beta-Binomial (binary outcomes)
    - Gamma-Poisson (count outcomes)
    - Normal-Normal (continuous outcomes)
    
- Once we have a posterior model, we must be able to apply the results.

    - Posterior estimation
    - Hypothesis testing
    - Prediction  

## Introduction {background-image="/Users/sseals/Library/CloudStorage/GoogleDrive-sseals@uwf.edu/My Drive/00 - Personal/R/quarto themes/slide.png" background-size="contain"} 

- Recall, we have the posterior pdf,

$$f(\theta|y) = \frac{f(\theta) L(\theta|y)}{f(y)} \propto f(\theta)L(\theta|y)$$
    
- Now, in the denominator, we need to remember,

$$f(y) = \int_{\theta_1} \int_{\theta_2} \cdot \cdot \cdot \int_{\theta_k} f(\theta) L(\theta|y) d\theta_k \cdot \cdot \cdot d\theta_2 d\theta_1$$

- Because this is ... not fun ... we will approximate the posterior via simulation.

## Introduction {background-image="/Users/sseals/Library/CloudStorage/GoogleDrive-sseals@uwf.edu/My Drive/00 - Personal/R/quarto themes/slide.png" background-size="contain"} 

- We are going to explore two simulation techniques:

    - grid approximation
    
    - Markov chain Monte Carlo (MCMC)
    
- Either method will produce a **sample** of $N$ values for $\theta$. 

$$\left \{ \theta^{(1)}, \theta^{(2)}, ..., \theta^{(N)} \right \}$$

- These $\theta_i$ will have properties that reflect those of the posterior model for $\theta$.

- To help us, we will apply these simulation techniques to Beta-Binomial and Gamma-Poisson models.

    - Note that these models do not require simulation! We know their posteriors!
    
    - That's why we are starting there -- we can link the concepts to what we know. :)

## Introduction {background-image="/Users/sseals/Library/CloudStorage/GoogleDrive-sseals@uwf.edu/My Drive/00 - Personal/R/quarto themes/slide.png" background-size="contain"} 

- Note: we will use the following packages that may be new to you:

    - `janitor`
    
    - `rstan`
    
    - `bayesplot`
    
- If you are using the server provided by HMCSE, *they have been installed for you*.

- If you are working at home, please check to see if you have the libraries, then install if you do not.

    - `install.packages(c("janitor", "rstan", "bayesplot"))`

## Grid Approximation {background-image="/Users/sseals/Library/CloudStorage/GoogleDrive-sseals@uwf.edu/My Drive/00 - Personal/R/quarto themes/slide.png" background-size="contain"} 

<center><img src = "images/W11-L1-a.png"></center>

- Suppose there is an image that you can't view in its entirety.

- We can see snippets along a grid that sweeps from left to right across the image. 

- The finer the grid, the clearer the image; if the grid is fine enough, the result is a good approximation.

## Grid Approximation {background-image="/Users/sseals/Library/CloudStorage/GoogleDrive-sseals@uwf.edu/My Drive/00 - Personal/R/quarto themes/slide.png" background-size="contain"} 

<center><img src = "images/W11-L1-a.png"></center>

- This is the general idea behind Bayesian grid approximation.

- Our target image is the posterior pdf, $f(\theta|y)$.

    - It is not necessary to observe all possible $f(\theta|y) \ \forall \theta$ for us to understand its structure.
    
    - Instead, we evaluate $f(\theta|y)$ at a finite, discrete grid of possible $\theta$ values.
    
    - Then, we take random samples from this discretized pdf to approximate the full posterior pdf.
    
## Grid Approximation {background-image="/Users/sseals/Library/CloudStorage/GoogleDrive-sseals@uwf.edu/My Drive/00 - Personal/R/quarto themes/slide.png" background-size="contain"} 

- Grid approximation produces a sample of $N$ independent $\theta$ values, $$\left\{ \theta^{(1)}, \theta^{(2)}, \theta^{(N)} \right\},$$ from a discretized approximation of the posterior pdf, $f(\theta|y)$.

- **Algorithm:**

    1. Define a discrete grid of possible $\theta$ values.
    
    2. Evaluate the prior pdf, $f(\theta)$, and the likelihood function, $L(\theta|y)$ at each $\theta$ grid value.
    
    3. Obtain a discrete approximation of the posterior pdf, $f(\theta|y)$ by:
    
        a. Calculating the product $f(\theta) L(\theta|y)$ at each $\theta$ grid value,
        
        b. Normalize the products from (a) to sum to 1 across all $\theta$.
        
    4. Randomly sample $N$ $\theta$ grid values with respect to their corresponding normalized posterior probabilities.        

## Grid Approximation - Example {background-image="/Users/sseals/Library/CloudStorage/GoogleDrive-sseals@uwf.edu/My Drive/00 - Personal/R/quarto themes/slide.png" background-size="contain"} 

- We will use the following Beta-Binomial model to learn how to do grid approximation:

$$
\begin{align*}
Y|\pi &\sim \text{Bin}(10, \pi) \\
\pi &\sim \text{Beta}(2, 2)
\end{align*}
$$

- Note that 

    - $Y$ is the number of successes in 10 independent trials.
    
    - Every trial has probability of success, $\pi$.
    
    - Our prior understanding about $\pi$ is captured by a $\text{Beta}(2,2)$ model.
    
- If we observe $Y = 9$ successes, we know that the updated posterior model for $\pi$ is $\text{Beta}(11, 3)$.

    - $Y + \alpha = 9+2$
    
    - $n - Y + \beta = 10-9+2$

## Grid Approximation  {background-image="/Users/sseals/Library/CloudStorage/GoogleDrive-sseals@uwf.edu/My Drive/00 - Personal/R/quarto themes/slide.png" background-size="contain"} 

- Instead of using the posterior we know, let's approximate it using grid approximation.

- **First step:** define a discrete grid of possible $\theta$ values.

    - So, let's consider $\pi \in \{0, 0.2, 0.4, 0.6, 0.8, 1\}$.
    
::: {.panel-tabset}

## Code

```{r}
#| eval: false
library(tidyverse)
grid_data <- tibble(pi_grid = seq(from = 0, to = 1, length = 6))
```

## Results

```{r}
#| echo: false
library(tidyverse)
(grid_data <- tibble(pi_grid = seq(from = 0, to = 1, length = 6)))
```

:::

## Grid Approximation  {background-image="/Users/sseals/Library/CloudStorage/GoogleDrive-sseals@uwf.edu/My Drive/00 - Personal/R/quarto themes/slide.png" background-size="contain"} 

- Instead of using the posterior we know, let's approximate it using grid approximation.

- **Second step:** evaluate the prior pdf, $f(\theta)$, and the likelihood function, $L(\theta|y)$ at each $\theta$ grid value.

    - We will use `dbeta()` and `dbinom()` to evaluate the $\text{Beta}(2,2)$ prior and $\text{Bin}(10, \pi)$ likelihood with $Y=9$ at each $\pi$ in `pi_grid`.
    
::: {.panel-tabset}   

## Code

```{r}
grid_data <- grid_data %>%
  mutate(prior = dbeta(pi_grid, 2, 2),
         likelihood = dbinom(9, 10, pi_grid))
```

## Results 

```{r}
#| echo: false
head(grid_data, n = 3)
```

:::

## Grid Approximation  {background-image="/Users/sseals/Library/CloudStorage/GoogleDrive-sseals@uwf.edu/My Drive/00 - Personal/R/quarto themes/slide.png" background-size="contain"} 

- Instead of using the posterior we know, let's approximate it using grid approximation.

- **Third step:** obtain a discrete approximation of the posterior pdf, $f(\theta|y)$ by calculating the product $f(\theta) L(\theta|y)$ at each $\theta$ grid value and normalizing the products to sum to 1 across all $\theta$.

::: {.panel-tabset}

## Code

```{r}
grid_data <- grid_data %>%
  mutate(unnormalized = likelihood*prior,
         posterior = unnormalized / sum(unnormalized))
```

## Results

```{r}
head(grid_data, n = 3)
```

## Verification

```{r}
grid_data %>%
  summarize(sum(unnormalized), sum(posterior))
```

:::

## Grid Approximation  {background-image="/Users/sseals/Library/CloudStorage/GoogleDrive-sseals@uwf.edu/My Drive/00 - Personal/R/quarto themes/slide.png" background-size="contain"} 

- Instead of using the posterior we know, let's approximate it using grid approximation.

- We now have a *glimpse* into the actual posterior pdf.

    - We can plot it to see what it looks like,
    
<center>    
```{r}
#| echo: false
ggplot(grid_data, aes(x = pi_grid, y = posterior)) + 
  geom_point() + 
  geom_segment(aes(x = pi_grid, xend = pi_grid, y = 0, yend = posterior)) +
  theme_bw()
```
</center>

## Grid Approximation  {background-image="/Users/sseals/Library/CloudStorage/GoogleDrive-sseals@uwf.edu/My Drive/00 - Personal/R/quarto themes/slide.png" background-size="contain"} 

- As we increase the number of possible $\theta$ values, the better we can "see" the resulting posterior.

::: {.panel-tabset}

## n = 6

<center>
```{r}
#| echo: false
tibble(pi_grid = seq(from = 0, to = 1, length = 6)) %>%
  mutate(prior = dbeta(pi_grid, 2, 2),
         likelihood = dbinom(9, 10, pi_grid),
         unnormalized = likelihood*prior,
         posterior = unnormalized / sum(unnormalized)) %>%
  ggplot(aes(x = pi_grid, y = posterior)) + 
  geom_point() + 
  geom_segment(aes(x = pi_grid, xend = pi_grid, y = 0, yend = posterior)) +
  theme_bw()
```
</center>

## n = 50

<center>
```{r}
#| echo: false
tibble(pi_grid = seq(from = 0, to = 1, length = 50)) %>%
  mutate(prior = dbeta(pi_grid, 2, 2),
         likelihood = dbinom(9, 10, pi_grid),
         unnormalized = likelihood*prior,
         posterior = unnormalized / sum(unnormalized)) %>%
  ggplot(aes(x = pi_grid, y = posterior)) + 
  geom_point() + 
  geom_segment(aes(x = pi_grid, xend = pi_grid, y = 0, yend = posterior)) +
  theme_bw()
```
</center>

## n = 100

<center>
```{r}
#| echo: false
tibble(pi_grid = seq(from = 0, to = 1, length = 100)) %>%
  mutate(prior = dbeta(pi_grid, 2, 2),
         likelihood = dbinom(9, 10, pi_grid),
         unnormalized = likelihood*prior,
         posterior = unnormalized / sum(unnormalized)) %>%
  ggplot(aes(x = pi_grid, y = posterior)) + 
  geom_point() + 
  geom_segment(aes(x = pi_grid, xend = pi_grid, y = 0, yend = posterior)) +
  theme_bw()
```
</center>

## n = 1000

<center>
```{r}
#| echo: false
tibble(pi_grid = seq(from = 0, to = 1, length = 1000)) %>%
  mutate(prior = dbeta(pi_grid, 2, 2),
         likelihood = dbinom(9, 10, pi_grid),
         unnormalized = likelihood*prior,
         posterior = unnormalized / sum(unnormalized)) %>%
  ggplot(aes(x = pi_grid, y = posterior)) + 
  geom_point() + 
  geom_segment(aes(x = pi_grid, xend = pi_grid, y = 0, yend = posterior)) +
  theme_bw()
```
</center>

## n = 10000

<center>
```{r}
#| echo: false
tibble(pi_grid = seq(from = 0, to = 1, length = 10000)) %>%
  mutate(prior = dbeta(pi_grid, 2, 2),
         likelihood = dbinom(9, 10, pi_grid),
         unnormalized = likelihood*prior,
         posterior = unnormalized / sum(unnormalized)) %>%
  ggplot(aes(x = pi_grid, y = posterior)) + 
  geom_point() + 
  geom_segment(aes(x = pi_grid, xend = pi_grid, y = 0, yend = posterior)) +
  theme_bw()
```
</center>

:::

## Grid Approximation - Example {background-image="/Users/sseals/Library/CloudStorage/GoogleDrive-sseals@uwf.edu/My Drive/00 - Personal/R/quarto themes/slide.png" background-size="contain"} 

- Your turn!

- Let's now apply this to a Gamma-Poisson model.

- $Y$ is the number of events that occur in a one-hour period, where events occur at an average rate of $\lambda$ per hour.

$$
\begin{align*}
Y_i | \lambda &\overset{\text{iid}}\sim \text{Pois}(\lambda) \\
\lambda &\sim \text{Gamma}(3,1)
\end{align*}
$$

- We collect two data points,  $(Y_1, Y_2) = (2, 3)$

- What is the posterior model?

- Simulate this posterior using grid approximation.

    - You will need `dgamma()` and `dpois()`.
    - Remember to set the seed.

## Markov chain Monte Carlo (MCMC) {background-image="/Users/sseals/Library/CloudStorage/GoogleDrive-sseals@uwf.edu/My Drive/00 - Personal/R/quarto themes/slide.png" background-size="contain"} 

- Markov chain Monte Carlo (MCMC) is an application of [Markov chains](https://en.wikipedia.org/wiki/Markov_chain) to simulate probability models.

- MCMC samples are not taken directly from the posterior pdf, $f(\theta | y)$... and they are not independent.

    - Each subsequent value depends on the previous value.
    
- Suppose we have an $N$-length MCMC sample, $$\left\{ \theta^{(1)}, \theta^{(2)}, \theta^{(3)}, ..., \theta^{(N)} \right\}$$

    - $\theta^{(2)}$ is drawn from a model that depends on $\theta^{(1)}$.

    - $\theta^{(3)}$ is drawn from a model that depends on $\theta^{(2)}$.
    
    - etc.

## Markov chain Monte Carlo (MCMC) {background-image="/Users/sseals/Library/CloudStorage/GoogleDrive-sseals@uwf.edu/My Drive/00 - Personal/R/quarto themes/slide.png" background-size="contain"}

- The $(i+1)$^st^ chain value, $\theta^{(i+1)}$ is drawn from a model that depends on data $y$ and the previous chain value, $\theta^{(i)}$.

$$f\left( \theta^{(i+1)} | \theta^{(i)}, y \right)$$ 
    
- It is important for us to note that the pdf from which a Markov chain is simulated is not equivalent to the posterior pdf!

$$f\left( \theta^{(i+1)} | \theta^{(i)}, y  \right) \ne f\left(\theta^{(i+1)}|y \right)$$

## Using `rstan` {background-image="/Users/sseals/Library/CloudStorage/GoogleDrive-sseals@uwf.edu/My Drive/00 - Personal/R/quarto themes/slide.png" background-size="contain"}

- We will use `rstan`:

    - define the Bayesian model structure in `rstan` notation
    
    - simulate the posterior
    
- Again, we will use the Beta-Binomial model from earlier.

    - `data`: in our example, $Y$ is the observed number of successes in 10 trials.
    
        - We need to tell `rstan` that $Y$ is an *integer* between 0 and 10.
        
    - `parameters`: in our example, our model depends on $\pi$.
    
        - We need to tell `rstan` that $\pi$ can be any *real* number between 0 and 1.
        
    - `model`: in our example, we need to specify $Y \sim \text{Bin}(10, \pi)$ and $\pi \sim \text{Beta}(2,2)$.


## Using `rstan` {background-image="/Users/sseals/Library/CloudStorage/GoogleDrive-sseals@uwf.edu/My Drive/00 - Personal/R/quarto themes/slide.png" background-size="contain"}

```{r}
#| echo: false

library(janitor)
library(bayesplot)
library(bayesrules)
library(rstan)
```

```{r}
# STEP 1: DEFINE the model
bb_model <- "
  data {
    int<lower = 0, upper = 10> Y;
  }
  parameters {
    real<lower = 0, upper = 1> pi;
  }
  model {
    Y ~ binomial(10, pi);
    pi ~ beta(2, 2);
  }
"
```

## Using `rstan` {background-image="/Users/sseals/Library/CloudStorage/GoogleDrive-sseals@uwf.edu/My Drive/00 - Personal/R/quarto themes/slide.png" background-size="contain"}

- Then, when we go to simulate, we first put in the model information

    - `model code`: the character string defining the model (in our case, `bb_model`).
    
    - `data`: a list of the observed data.
    
        - In this example, we are using $Y = 9$ - a single data point.
        
- Then, we put in the Markov chain information,

    - `chains`: how many parallel Markov chains to run.
    
        - This will be the number of distinct $\theta$ values we want.
        
    - `iter`: desired number of iterations, or length of Markov chain.
    
        - Half are thrown out as "burn in" samples. 
        
            - "burn in"? Think: pancakes!
        
    - `seed`: used to set the seed of the RNG.        

## Using `rstan` {background-image="/Users/sseals/Library/CloudStorage/GoogleDrive-sseals@uwf.edu/My Drive/00 - Personal/R/quarto themes/slide.png" background-size="contain"}

::: {.panel-tabset}

## Code

```{r}
#| eval: false
# STEP 2: SIMULATE the posterior
bb_sim <- stan(model_code = bb_model, data = list(Y = 9), 
               chains = 4, iter = 5000*2, seed = 84735)
```

## Output

```{r}
#| echo: false
# STEP 2: SIMULATE the posterior
bb_sim <- stan(model_code = bb_model, data = list(Y = 9), 
               chains = 4, iter = 5000*2, seed = 84735)
```

:::

## Using `rstan` {background-image="/Users/sseals/Library/CloudStorage/GoogleDrive-sseals@uwf.edu/My Drive/00 - Personal/R/quarto themes/slide.png" background-size="contain"}

- Now, we need to extract the values,

```{r}
as.array(bb_sim, pars = "pi") %>% head(4)
```

- Remember, these are *not* a random sample from the posterior!

- They are also *not* independent!

- Each chain forms a dependent 5,000 length Markov chain of $\left\{ \pi^{(1)}, \pi^{(2)}, ..., \pi^{(5000)}\right\}$

    - Each chain will move along the sample space of plausible values for $\pi$.
    
## Using `rstan` {background-image="/Users/sseals/Library/CloudStorage/GoogleDrive-sseals@uwf.edu/My Drive/00 - Personal/R/quarto themes/slide.png" background-size="contain"}

- We will look at the trace plot (using `mcmc_trace()` from `bayesplot` package) to see what the values did longitudinally.

<center>
```{r}
mcmc_trace(bb_sim, pars = "pi", size = 0.1)
```
</center>

## Using `rstan` {background-image="/Users/sseals/Library/CloudStorage/GoogleDrive-sseals@uwf.edu/My Drive/00 - Personal/R/quarto themes/slide.png" background-size="contain"}

- We can also look at the `mcmc_hist()` function,

::: {.panel-tabset}

## Code

```{r}
#| eval: false
mcmc_hist(bb_sim, pars = "pi") +
  yaxis_text(TRUE) +
  ylab("count")
```

## Graph

<center>
```{r}
#| echo: false
mcmc_hist(bb_sim, pars = "pi") +
  yaxis_text(TRUE) +
  ylab("count")
```
</center>

:::

## Using `rstan` {background-image="/Users/sseals/Library/CloudStorage/GoogleDrive-sseals@uwf.edu/My Drive/00 - Personal/R/quarto themes/slide.png" background-size="contain"}

- We can also look at the `mcmc_dens()` function,

::: {.panel-tabset}

## Code

```{r}
#| eval: false
mcmc_dens(bb_sim, pars = "pi") +
  yaxis_text(TRUE) +
  ylab("count")
```

## Graph

<center>
```{r}
#| echo: false
mcmc_dens(bb_sim, pars = "pi") +
  yaxis_text(TRUE) +
  ylab("count")
```
</center>

:::

## MCMC - Example {background-image="/Users/sseals/Library/CloudStorage/GoogleDrive-sseals@uwf.edu/My Drive/00 - Personal/R/quarto themes/slide.png" background-size="contain"} 

- Your turn!

- Let's now apply this to a Gamma-Poisson model.

- $Y$ is the number of events that occur in a one-hour period, where events occur at an average rate of $\lambda$ per hour.

$$
\begin{align*}
Y_i | \lambda &\overset{\text{iid}}\sim \text{Pois}(\lambda) \\
\lambda &\sim \text{Gamma}(3,1)
\end{align*}
$$

- We collect two data points, $(Y_1, Y_2) = (2, 8)$

- What is the posterior model?

- Simulate this posterior using MCMC in `rstan`.

- If you struggle, ask Dr. Seals for a hint :)










